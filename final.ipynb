{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    # Actor provides the next action to take\n",
    "    def __init__(self, state_dim, action_dim, limit):\n",
    "        super(Actor, self).__init__()\n",
    "        self.limit = torch.FloatTensor(limit)\n",
    "\n",
    "        self.fc1 = nn.Linear(state_dim, 400)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "\n",
    "        self.fc2 = nn.Linear(400, 300)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "        self.fc3 = nn.Linear(300, action_dim)\n",
    "        nn.init.uniform_(self.fc3.weight, -0.003, 0.003)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    # Critic estimates the state-value function Q(S, A)\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 400)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "\n",
    "        self.fc2 = nn.Linear(400 + action_dim, 300)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "        self.fc3 = nn.Linear(300, 1)\n",
    "        nn.init.uniform_(self.fc3.weight, -0.003, 0.003)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        s = F.relu(self.fc1(state))\n",
    "        x = torch.cat((s, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAU = 0.05\n",
    "A_LEARNING_RATE = 1e-4\n",
    "C_LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import NamedTuple, List\n",
    "\n",
    "\n",
    "class Experience(NamedTuple):\n",
    "    \"\"\"\n",
    "    An experience contains the data of one Agent transition.\n",
    "    - Observation\n",
    "    - Action\n",
    "    - Reward\n",
    "    - Done flag\n",
    "    - Next Observation\n",
    "    \"\"\"\n",
    "\n",
    "    obs: np.ndarray\n",
    "    action: np.ndarray\n",
    "    reward: float\n",
    "    done: bool\n",
    "    next_obs: np.ndarray\n",
    "\n",
    "# A Trajectory is an ordered sequence of Experiences\n",
    "Trajectory = List[Experience]\n",
    "\n",
    "# A Buffer is an unordered list of Experiences from multiple Trajectories\n",
    "Buffer = List[Experience]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Orn_Uhlen:\n",
    "    def __init__(self, n_actions, mu=0, theta=0.15, sigma=0.2):\n",
    "        self.n_actions = n_actions\n",
    "        self.X = np.ones(n_actions) * mu\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.theta = theta\n",
    "\n",
    "    def reset(self):\n",
    "        self.X = np.ones(self.n_actions) * self.mu\n",
    "\n",
    "    def sample(self):\n",
    "        dX = self.theta * (self.mu - self.X)\n",
    "        dX += self.sigma * np.random.randn(self.n_actions)\n",
    "        self.X += dX\n",
    "        return self.X\n",
    "\n",
    "class DDPG:\n",
    "    def __init__(self, state_dim, n_actions, limit):\n",
    "        self.env = env\n",
    "\n",
    "        n_inp = state_dim\n",
    "        n_out = n_actions\n",
    "        a_limit = limit\n",
    "\n",
    "        self.actor_net = Actor(n_inp, n_out, a_limit).to(device)\n",
    "        self.critic_net = Critic(n_inp, n_out).to(device)\n",
    "\n",
    "        self.target_actor_net = Actor(n_inp, n_out, a_limit).to(device)\n",
    "        self.target_critic_net = Critic(n_inp, n_out).to(device)\n",
    "\n",
    "        if os.path.exists(actor_model_path):\n",
    "            self.actor_net.load_state_dict(torch.load(actor_model_path))\n",
    "\n",
    "        if os.path.exists(critic_model_path):\n",
    "            self.critic_net.load_state_dict(torch.load(critic_model_path))\n",
    "\n",
    "        self.target_actor_net.load_state_dict(self.actor_net.state_dict())\n",
    "        self.target_critic_net.load_state_dict(self.critic_net.state_dict())\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.actor_net.parameters(), lr=A_LEARNING_RATE)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_net.parameters(), lr=C_LEARNING_RATE)\n",
    "\n",
    "        self.noise = Orn_Uhlen(n_out)\n",
    "        \n",
    "    def update_network(self, buffer: Buffer):\n",
    "        BATCH_SIZE = 1000\n",
    "        NUM_EPOCH = 3\n",
    "        GAMMA = 0.9\n",
    "        \n",
    "        batch_size = min(len(buffer), BATCH_SIZE)\n",
    "        random.shuffle(buffer)\n",
    "        \n",
    "        batches = [\n",
    "            buffer[batch_size * start : batch_size * (start + 1)]\n",
    "            for start in range(int(len(buffer) / batch_size))\n",
    "        ]\n",
    "        \n",
    "        for _ in range(NUM_EPOCH):\n",
    "            for batch in batches:\n",
    "                obs = torch.from_numpy(np.stack([el.obs for el in batch])).float().to(device)\n",
    "                action = torch.from_numpy(np.stack([el.action for el in batch])).float().to(device)\n",
    "                reward = torch.from_numpy(\n",
    "                    np.array([el.reward for el in batch], dtype=np.float32).reshape(-1, 1)\n",
    "                ).to(device)\n",
    "                done = torch.from_numpy(\n",
    "                     np.array([el.done for el in batch], dtype=np.float32).reshape(-1, 1)\n",
    "                ).to(device)\n",
    "                next_obs = torch.from_numpy(np.stack([el.next_obs for el in batch])).float().to(device)\n",
    "                \n",
    "                A_critic = self.target_actor_net(next_obs)\n",
    "                Q_Spr_A = self.target_critic_net(next_obs, A_critic).detach()\n",
    "                target_y = reward + GAMMA * Q_Spr_A * (1 - done)\n",
    "                y = self.critic_net(obs, action)\n",
    "                \n",
    "                # prediction loss for critic\n",
    "                critic_loss = torch.mean(torch.pow(y - target_y, 2))\n",
    "                \n",
    "                # update critic network -> Q(S, A)\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                self.critic_optimizer.step()\n",
    "                \n",
    "                # find actor loss\n",
    "                A_actor = self.actor_net(obs)\n",
    "                actor_loss = -1 * torch.mean(self.critic_net(obs, A_actor))\n",
    "                \n",
    "                # update actor network\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                # soft_update target networks\n",
    "                self.soft_update()\n",
    "            self.noise.reset()\n",
    "                \n",
    "    def get_actions(self, observations, add_noise):\n",
    "        obs = torch.from_numpy(np.stack(observations)).to(device)\n",
    "        pred = self.actor_net(obs).detach()\n",
    "        if add_noise:\n",
    "            noise = self.noise.sample()\n",
    "            action = (pred.data.cpu().numpy() + noise)[:]\n",
    "            return action\n",
    "        else:\n",
    "            return (pred.data.cpu().numpy())[:]\n",
    "\n",
    "    def soft_update(self):\n",
    "        for target, src in zip(self.target_actor_net.parameters(), self.actor_net.parameters()):\n",
    "            target.data.copy_(target.data * (1.0 - TAU) + src.data * TAU)\n",
    "\n",
    "        for target, src in zip(self.target_critic_net.parameters(), self.critic_net.parameters()):\n",
    "            target.data.copy_(target.data * (1.0 - TAU) + src.data * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import BaseEnv\n",
    "from typing import Dict\n",
    "import random\n",
    "\n",
    "\n",
    "def generate_trajectories(\n",
    "    env: BaseEnv,\n",
    "    prey_network: DDPG,\n",
    "    pred_network: DDPG,\n",
    "    buffer_size: int,\n",
    "    add_noise: bool,\n",
    "):\n",
    "    buffer_prey: Buffer = []\n",
    "    buffer_pred: Buffer = []\n",
    "    numresets = 0\n",
    "\n",
    "    # Reset the environment\n",
    "    env.reset()\n",
    "    \n",
    "    # Read and store the Behavior Name of the Environment\n",
    "    behavior_name_prey = list(env.behavior_specs)[0]\n",
    "    behavior_name_pred = list(env.behavior_specs)[1]\n",
    "    # Read and store the Behavior Specs of the Environment\n",
    "    spec_prey = env.behavior_specs[behavior_name_prey]\n",
    "    spec_pred = env.behavior_specs[behavior_name_pred]\n",
    "\n",
    "    # Mapping from AgentId to Trajectories.\n",
    "    # For creating trajectories for each agent\n",
    "    dict_trajectories_from_prey: Dict[int, Trajectory] = {}\n",
    "    # Last observation of each Agent\n",
    "    dict_last_obs_from_prey: Dict[int, np.ndarray] = {}\n",
    "    # Last acvtion of each Agent\n",
    "    dict_last_action_from_prey: Dict[int, np.ndarray] = {}\n",
    "    # Cumulative rewards of each Agent (Only for reporting)\n",
    "    dict_cumulative_reward_from_prey: Dict[int, float] = {}\n",
    "    # List for storing cumulative rewards\n",
    "    cumulative_rewards_prey: List[float] = []\n",
    "\n",
    "    # Same mappings as above but for Predator\n",
    "    dict_trajectories_from_pred: Dict[int, Trajectory] = {}\n",
    "    dict_last_obs_from_pred: Dict[int, np.ndarray] = {}\n",
    "    dict_last_action_from_pred: Dict[int, np.ndarray] = {}\n",
    "    dict_cumulative_reward_from_pred: Dict[int, float] = {}\n",
    "    cumulative_rewards_pred: List[float] = []\n",
    "\n",
    "    while (\n",
    "        len(buffer_prey) < buffer_size or len(buffer_pred) < buffer_size\n",
    "    ):  # While not enough experience for both prey and predator\n",
    "\n",
    "        # Get the Decision Steps and Terminal Steps of Prey And Predator\n",
    "        decision_steps_prey, terminal_steps_prey = env.get_steps(behavior_name_prey)\n",
    "        decision_steps_pred, terminal_steps_pred = env.get_steps(behavior_name_pred)\n",
    "\n",
    "        # For all PREYS with a Terminal Step:\n",
    "        for agent_id_terminated_prey in terminal_steps_prey.agent_id:\n",
    "            try:\n",
    "                # Create its last experience (is last because the Agent terminated)\n",
    "                last_experience = Experience(\n",
    "                    obs=dict_last_obs_from_prey[agent_id_terminated_prey].copy(),\n",
    "                    reward=terminal_steps_prey[agent_id_terminated_prey].reward,\n",
    "                    done=not terminal_steps_prey[agent_id_terminated_prey].interrupted,\n",
    "                    action=dict_last_action_from_prey[agent_id_terminated_prey].copy(),\n",
    "                    next_obs=terminal_steps_prey[agent_id_terminated_prey].obs[0],\n",
    "                )\n",
    "                # Clear its last observation and action (Since the trajectory is over)\n",
    "                dict_last_obs_from_prey.pop(agent_id_terminated_prey)\n",
    "                dict_last_action_from_prey.pop(agent_id_terminated_prey)\n",
    "                # Cumulative reward\n",
    "                cumulative_reward = (\n",
    "                    dict_cumulative_reward_from_prey.pop(agent_id_terminated_prey)\n",
    "                    + terminal_steps_prey[agent_id_terminated_prey].reward\n",
    "                )\n",
    "                cumulative_rewards_prey.append(cumulative_reward)\n",
    "                # Add the Trajectory and the last experience to the buffer\n",
    "                buffer_prey.extend(\n",
    "                    dict_trajectories_from_prey.pop(agent_id_terminated_prey)\n",
    "                )\n",
    "                buffer_prey.append(last_experience)\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "\n",
    "        # For all PREDS with a Terminal Step:\n",
    "        for agent_id_terminated_pred in terminal_steps_pred.agent_id:\n",
    "            try:\n",
    "                # Create its last experience (is last because the Agent terminated)\n",
    "                last_experience = Experience(\n",
    "                    obs=dict_last_obs_from_pred[agent_id_terminated_pred].copy(),\n",
    "                    reward=terminal_steps_pred[agent_id_terminated_pred].reward,\n",
    "                    done=not terminal_steps_pred[agent_id_terminated_pred].interrupted,\n",
    "                    action=dict_last_action_from_pred[agent_id_terminated_pred].copy(),\n",
    "                    next_obs=terminal_steps_pred[agent_id_terminated_pred].obs[0],\n",
    "                )\n",
    "                # Clear its last observation and action (Since the trajectory is over)\n",
    "                dict_last_obs_from_pred.pop(agent_id_terminated_pred)\n",
    "                dict_last_action_from_pred.pop(agent_id_terminated_pred)\n",
    "                # Cumulative reward\n",
    "                cumulative_reward = (\n",
    "                    dict_cumulative_reward_from_pred.pop(agent_id_terminated_pred)\n",
    "                    + terminal_steps_pred[agent_id_terminated_pred].reward\n",
    "                )\n",
    "                cumulative_rewards_pred.append(cumulative_reward)\n",
    "                # Add the Trajectory and the last experience to the buffer\n",
    "                buffer_pred.extend(\n",
    "                    dict_trajectories_from_pred.pop(agent_id_terminated_pred)\n",
    "                )\n",
    "                buffer_pred.append(last_experience)\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "\n",
    "        # For all PREYS with a Decision Step:\n",
    "        for agent_id_decisions_prey in decision_steps_prey.agent_id:\n",
    "            # If the Agent does not have a Trajectory, create an empty one\n",
    "            if agent_id_decisions_prey not in dict_trajectories_from_prey:\n",
    "                dict_trajectories_from_prey[agent_id_decisions_prey] = []\n",
    "                dict_cumulative_reward_from_prey[agent_id_decisions_prey] = 0\n",
    "\n",
    "            # If the Agent requesting a decision has a \"last observation\"\n",
    "            if agent_id_decisions_prey in dict_last_obs_from_prey:\n",
    "                # Create an Experience from the last observation and the Decision Step\n",
    "                exp = Experience(\n",
    "                    obs=dict_last_obs_from_prey[agent_id_decisions_prey].copy(),\n",
    "                    reward=decision_steps_prey[agent_id_decisions_prey].reward,\n",
    "                    done=False,\n",
    "                    action=dict_last_action_from_prey[agent_id_decisions_prey].copy(),\n",
    "                    next_obs=decision_steps_prey[agent_id_decisions_prey].obs[0],\n",
    "                )\n",
    "                # Update the Trajectory of the Agent and its cumulative reward\n",
    "                dict_trajectories_from_prey[agent_id_decisions_prey].append(exp)\n",
    "                dict_cumulative_reward_from_prey[\n",
    "                    agent_id_decisions_prey\n",
    "                ] += decision_steps_prey[agent_id_decisions_prey].reward\n",
    "                # Store the observation as the new \"last observation\"\n",
    "            dict_last_obs_from_prey[agent_id_decisions_prey] = decision_steps_prey[\n",
    "                agent_id_decisions_prey\n",
    "            ].obs[0]\n",
    "\n",
    "        # Generate an action for all the PREYS that requested a decision\n",
    "        # Compute the values for each action given the observation\n",
    "        # Send actions to environment to be used in next step\n",
    "        if len(decision_steps_prey) != 0:\n",
    "            actions_prey = prey_network.get_actions(\n",
    "                decision_steps_prey.obs[0], add_noise\n",
    "            ).copy()\n",
    "\n",
    "            for agent_index, agent_id in enumerate(decision_steps_prey.agent_id):\n",
    "                dict_last_action_from_prey[agent_id] = actions_prey[agent_index]\n",
    "\n",
    "            env.set_actions(behavior_name_prey, np.asarray(actions_prey))\n",
    "\n",
    "        # For all PREDS with a Decision Step:\n",
    "        for agent_id_decisions_pred in decision_steps_pred.agent_id:\n",
    "            # If the Agent does not have a Trajectory, create an empty one\n",
    "            if agent_id_decisions_pred not in dict_trajectories_from_pred:\n",
    "                dict_trajectories_from_pred[agent_id_decisions_pred] = []\n",
    "                dict_cumulative_reward_from_pred[agent_id_decisions_pred] = 0\n",
    "\n",
    "            # If the Agent requesting a decision has a \"last observation\"\n",
    "            if agent_id_decisions_pred in dict_last_obs_from_pred:\n",
    "                # Create an Experience from the last observation and the Decision Step\n",
    "                exp = Experience(\n",
    "                    obs=dict_last_obs_from_pred[agent_id_decisions_pred].copy(),\n",
    "                    reward=decision_steps_pred[agent_id_decisions_pred].reward,\n",
    "                    done=False,\n",
    "                    action=dict_last_action_from_pred[agent_id_decisions_pred].copy(),\n",
    "                    next_obs=decision_steps_pred[agent_id_decisions_pred].obs[0],\n",
    "                )\n",
    "                # Update the Trajectory of the Agent and its cumulative reward\n",
    "                dict_trajectories_from_pred[agent_id_decisions_pred].append(exp)\n",
    "                dict_cumulative_reward_from_pred[\n",
    "                    agent_id_decisions_pred\n",
    "                ] += decision_steps_pred[agent_id_decisions_pred].reward\n",
    "                # Store the observation as the new \"last observation\"\n",
    "            dict_last_obs_from_pred[agent_id_decisions_pred] = decision_steps_pred[\n",
    "                agent_id_decisions_pred\n",
    "            ].obs[0]\n",
    "\n",
    "\n",
    "        # Generate an action for all the PREDS that requested a decision\n",
    "        # Compute the values for each action given the observation\n",
    "        # Send actions to environment to be used in next step\n",
    "        if len(decision_steps_pred) != 0:\n",
    "            actions_pred = pred_network.get_actions(\n",
    "                decision_steps_pred.obs[0], add_noise\n",
    "            ).copy()\n",
    "\n",
    "            for agent_index, agent_id in enumerate(decision_steps_pred.agent_id):\n",
    "                dict_last_action_from_pred[agent_id] = actions_pred[agent_index]\n",
    "\n",
    "            env.set_actions(behavior_name_pred, np.asarray(actions_pred))\n",
    "\n",
    "        # Step the simulation\n",
    "        env.step()\n",
    "\n",
    "    return (\n",
    "        buffer_prey,\n",
    "        np.mean(cumulative_rewards_prey),\n",
    "        buffer_pred,\n",
    "        np.mean(cumulative_rewards_pred),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mlagents_envs.registry import default_registry\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "channel = EngineConfigurationChannel()\n",
    "env = UnityEnvironment(file_name=\"t/UnityEnvironment.exe\", worker_id=100, side_channels=[channel])\n",
    "channel.set_configuration_parameters(time_scale = 1.0)\n",
    "print(\"Environment created.\")\n",
    "\n",
    "env.reset()\n",
    "\n",
    "n_actions = 3\n",
    "state_dim_prey = 27\n",
    "state_dim_pred = 18\n",
    "limit = 1\n",
    "\n",
    "\n",
    "# Prey and Predator networks\n",
    "prey = DDPG(state_dim_prey, n_actions, limit)\n",
    "pred = DDPG(state_dim_pred, n_actions, limit)\n",
    "\n",
    "# Buffer for storing memories\n",
    "experiences_prey: Buffer = []\n",
    "experiences_pred: Buffer = []\n",
    "\n",
    "# For reporting\n",
    "cumulative_rewards_prey: List[float] = []\n",
    "cumulative_rewards_pred: List[float] = []\n",
    "\n",
    "# number of training steps\n",
    "NUM_TRAINING_STEPS = 75\n",
    "# number of experiences to collect at each training step\n",
    "NUM_NEW_EXP = 10000\n",
    "# The maximum size of the Memory\n",
    "BUFFER_SIZE = 100000\n",
    "\n",
    "\n",
    "for n in range(NUM_TRAINING_STEPS):\n",
    "    # Generate experiences for agents\n",
    "    prey_exp, _, pred_exp, _ = generate_trajectories(env, prey, pred, NUM_NEW_EXP, True)\n",
    "    \n",
    "    random.shuffle(experiences_prey)\n",
    "    random.shuffle(experiences_pred)\n",
    "    \n",
    "    if len(experiences_prey) > BUFFER_SIZE:\n",
    "        experiences_prey = experiences_prey[:BUFFER_SIZE]\n",
    "    if len(experiences_pred) > BUFFER_SIZE:\n",
    "        experiences_pred = experiences_pred[:BUFFER_SIZE]\n",
    "    \n",
    "    # Add newly generated experiences to the Memory and Update the networks will all experiences\n",
    "    experiences_prey.extend(prey_exp)\n",
    "    prey.update_network(experiences_prey)\n",
    "    \n",
    "    experiences_pred.extend(pred_exp)\n",
    "    pred.update_network(experiences_pred)\n",
    "    \n",
    "    \n",
    "    # Run simulation with updated networks to collect rewards\n",
    "    _, rewards_prey, _, rewards_pred = generate_trajectories(env, prey, pred, 1000, False)\n",
    "    cumulative_rewards_prey.append(rewards_prey)\n",
    "    cumulative_rewards_pred.append(rewards_pred)\n",
    "    \n",
    "    print(\"Training step \", n+1, \"\\tPrey Reward: \", rewards_prey, \"\\tPred Reward: \", rewards_pred)\n",
    "\n",
    "# Save the models\n",
    "torch.save(prey.actor_net.state_dict(), \"prey2_actor.pth\")\n",
    "torch.save(pred.actor_net.state_dict(), \"pred2_actor.pth\")\n",
    "torch.save(prey.critic_net.state_dict(), \"prey2_crit.pth\")\n",
    "torch.save(pred.critic_net.state_dict(), \"pred2_crit.pth\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cumulative Rewards')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Trainin Plot\n",
    "plt.plot(range(NUM_TRAINING_STEPS), cumulative_rewards_prey, \"b\", label='Prey')\n",
    "plt.plot(range(NUM_TRAINING_STEPS), cumulative_rewards_pred, \"r\", label='Predator')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel(\"#Training Step\")\n",
    "plt.ylabel(\"Cumulative Rewards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
